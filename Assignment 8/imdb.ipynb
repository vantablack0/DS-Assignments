{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6997fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers  \n",
    "from transformers import DistilBertModel\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f7b3661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffcac312",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = [f for f in listdir(\"aclImdb/train/pos\") if isfile(join(\"aclImdb/train/pos\", f))]\n",
    "neg_train = [f for f in listdir(\"aclImdb/train/neg\") if isfile(join(\"aclImdb/train/neg\", f))]\n",
    "pos_test = [f for f in listdir(\"aclImdb/test/pos\") if isfile(join(\"aclImdb/test/pos\", f))]\n",
    "neg_test = [f for f in listdir(\"aclImdb/test/neg\") if isfile(join(\"aclImdb/test/neg\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ca7726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pos_train[0].split(\"_\")[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e9e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = []\n",
    "neg_labels = []\n",
    "pos_reviews = []\n",
    "neg_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbb532e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pos_train:\n",
    "    pos_labels.append(1)\n",
    "    filepath = \"aclImdb/train/pos/\" + file\n",
    "    with open(filepath) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = lines[0]\n",
    "    pos_reviews.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bc46f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in neg_train:\n",
    "    neg_labels.append(0)\n",
    "    filepath = \"aclImdb/train/neg/\" + file\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = lines[0]\n",
    "    neg_reviews.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4565f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pos_reviews + neg_reviews\n",
    "labels = pos_labels + neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c6a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imdb(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index],self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4822e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = imdb(reviews,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbcbaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10f105f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33ff8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # add your additional layers here, for example a dropout layer followed by a linear classification head\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        sequence_output = self.bert(\n",
    "               ids, \n",
    "               attention_mask=mask,     \n",
    "          )\n",
    "            \n",
    "        # we apply dropout to the sequence output, tensor has shape (batch_size, sequence_length, 768)\n",
    "        sequence_output = self.dropout(sequence_output[0])\n",
    "        \n",
    "    \n",
    "        # next, we apply the linear layer. The linear layer (which applies a linear transformation)\n",
    "        # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to\n",
    "        # a single token in the input sequence) and outputs 2 numbers (scores, or logits) for every token\n",
    "        # so the logits are of shape (batch_size, sequence_length, 2)\n",
    "        logits = self.out(sequence_output)\n",
    "        print(logits.shape)\n",
    "        logits = torch.squeeze(logits)\n",
    "        logits = torch.mean(logits,1)\n",
    "    \n",
    "        logits = F.sigmoid(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a70b9427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dist_model = CustomBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76483172",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.SGD(dist_model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8061d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3a312be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Using device:', device)\n",
    "dist_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb85d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for j,(x_train,y_train) in enumerate(train_data): ## If you have a DataLoader()  object to get the data.\n",
    "\n",
    "        data = list(x_train)\n",
    "        \n",
    "        dist_model.train()\n",
    "        targets = y_train ## assuming that data loader returns a tuple of data and its targets\n",
    "        optimizer.zero_grad()   \n",
    "        encoding = tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True, truncation=True,max_length=512, add_special_tokens = True)\n",
    "       \n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        input_ids.to(device)\n",
    "        attention_mask.to(device)\n",
    "        outputs = dist_model(input_ids,attention_mask)\n",
    "        \n",
    "        #outputs = F.log_softmax(outputs, dim=1)\n",
    "        #print(outputs)\n",
    "        #print(outputs.shape)\n",
    "        #outputs = torch.squeeze(outputs)\n",
    "        loss = loss_fn(outputs, targets.float())\n",
    "        print(f\"Epoch {epoch+1} , LOSS :  {loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f754b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
